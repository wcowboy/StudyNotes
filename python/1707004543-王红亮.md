# 爬虫实战 1：爬取北京地区短租房信息

## 1. 简介

用python + requests + BeautifulSoup 北京地区短租房信息。包括标题，地址，价格，房东名称，性别，头像链接

## 2.  URL构造

打开[小猪短租北京首页]([https://bj.xiaozhu.com](https://bj.xiaozhu.com/))：可以看到主页如下图：

![](https://wcowboy-1258563652.cos.ap-chengdu.myqcloud.com/python/1.png)

手动翻页浏览前几页观察URL：

```
<https://bj.xiaozhu.com/>
<https://bj.xiaozhu.com/search-duanzufang-p2-0/>
<https://bj.xiaozhu.com/search-duanzufang-p3-0/>
```

发现更改p后面的数字好像可以访问不同的页，将其改成p1发现也可以访问第一页，然后就可以构造全部的13页网址

```python
urls = ['http://bj.xiaozhu.com/search-duanzufang-p{}-0/'.format(number) for number in range(1, 14)]
```

## 3. 获取房源连接

打开开发者工具可以看到对应的所有房子的链接都在<ul>表签下，只需要遍历一遍就可以获取每一个房源的链接

![](https://wcowboy-1258563652.cos.ap-chengdu.myqcloud.com/python/2.png)

## 4. 获取房源对应信息

点开一个房源，复制如下信息

```python
# tittles ：#body > div.wrap.clearfix.con_bg > div.con_l > div.pho_info > h4 > em
# addresses ：#body > div.wrap.clearfix.con_bg > div.con_l > div.pho_info > p > span
# prices ：#span.pr5
# imgs ：#floatRightBox > div.js_box.clearfix > div.member_pic > a > img
# names ：#floatRightBox > div.js_box.clearfix > div.w_240 > h6 > a
# sexs ：#floatRightBox > div.js_box.clearfix > div.member_pic > div
```

判断性别: 发现女性

```html
 <span class = ”member_girl_ico"> </span>
```

构造判断性别的函数:根据类名判断

```python
def judgment_sex(class_name):
    if class_name == ['member_ico1']:
        return '女'
    else:
        return '男'
```

## 5. 信息储存

信息临时用list保存list写入文件，因为有些汉字没有编码可能无法写入，写入文件时加一个UnicodeEncodeError

## 6.最终结果

![](https://wcowboy-1258563652.cos.ap-chengdu.myqcloud.com/python/3.png)![](https://wcowboy-1258563652.cos.ap-chengdu.myqcloud.com/python/4.png)

## 7.具体代码

```python
from bs4 import BeautifulSoup # 解析request请求的网页，将源码解析成Soup文档
import requests				 # 获取网页数据
import time					 # 调用延时函数

# 请求头，伪装浏览器
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'
}
# 打开txt文档
f = open('xiaozhuduanzhu.txt', 'w+')

# 定义性别判断函数
def judgment_sex(class_name):
    if class_name == ['member_ico1']:
        return '女'
    else:
        return '男'

# 根据每一页的url获取每一个房源的url
def get_links(url):
    wb_data = requests.get(url, headers=headers)
    soup = BeautifulSoup(wb_data.text, 'lxml')
    links = soup.select('#page_list > ul > li > a')
    for link in links:								# 遍历房源信息
        href = link.get("href")
        get_info(href)

# 获取每个房源的信息
def get_info(url):
    wb_data = requests.get(url, headers=headers)	#请求
    soup = BeautifulSoup(wb_data.text, 'lxml')
    # 信息获取
    tittles = soup.select('div.wrap.clearfix.con_bg > div.con_l > div.pho_info > h4 >em')
    addresses = soup.select('span.pr5')
    prices = soup.select('#pricePart > div.day_l > span')
    imgs = soup.select('#floatRightBox > div.js_box.clearfix > div.member_pic > a > img')
    names = soup.select('#floatRightBox > div.js_box.clearfix > div.w_240 > h6 > a')
    sexs = soup.select('#floatRightBox > div.js_box.clearfix > div.member_pic > div')
    #遍历
    for tittle, address, price, img, name, sex in zip(tittles, addresses, prices, imgs, names, sexs):
        
        data = {
            'tittle': tittle.get_text().strip(), # 去除空格
            'address': address.get_text().strip(),
            'price': price.get_text(),			#转文本
            'img': img.get("src"),
            'name': name.get_text(),			# 获取图片链接
            'sex': judgment_sex(sex.get("class"))
        }
        #写数据
        try:
            f.write(str(data))
            f.write('\n')
            print(data)
        except UnicodeEncodeError:
            print('error')


if __name__ == '__main__':
    urls = ['http://bj.xiaozhu.com/search-duanzufang-p{}-0/'.format(number) for number in range(1, 14)]
    for single_url in urls:
        get_links(single_url)
        time.sleep(2)
    f.close()

```

